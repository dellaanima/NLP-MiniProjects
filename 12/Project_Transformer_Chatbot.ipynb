{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dellaanima/NLP_GoingDeeper/blob/main/12/Project_Transformer_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "centered-standard",
      "metadata": {
        "id": "centered-standard"
      },
      "source": [
        "# Project: Transformer 로 챗봇 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sought-diameter",
      "metadata": {
        "id": "sought-diameter"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 데이터 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meaningful-drain",
      "metadata": {
        "id": "meaningful-drain"
      },
      "outputs": [],
      "source": [
        "import re   #정규식\n",
        "import random   #데이터 증강용 난수\n",
        "import numpy as np   #행렬 연산\n",
        "import pandas as pd   #데이터프레임\n",
        "import tensorflow as tf   #신경망\n",
        "import matplotlib.pyplot as plt   #데이터 시각화\n",
        "from tqdm.notebook import tqdm   #학습 과정 출력\n",
        "\n",
        "\n",
        "import gensim   #Word2Vec\n",
        "from konlpy.tag import Mecab   #형태소 분석\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu   #bleu 연산\n",
        "from nltk.translate.bleu_score import SmoothingFunction   #bleu 연산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "contained-community",
      "metadata": {
        "id": "contained-community",
        "outputId": "6b7bbc85-13b2-44f8-e8ab-1167bb12e454"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8725</th>\n",
              "      <td>헤이진지 이제 딱 한달</td>\n",
              "      <td>후폭풍이 지나갔길 바랄게요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11697</th>\n",
              "      <td>친구들이 짝녀를 별로라고 하네요.</td>\n",
              "      <td>무슨 이유에서 그랬을까요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4928</th>\n",
              "      <td>피자 막막 먹고 싶어.</td>\n",
              "      <td>배달 시키세요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2172</th>\n",
              "      <td>불 날 뻔했어</td>\n",
              "      <td>조심하세요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11707</th>\n",
              "      <td>친구의 남친 좋아하게 된 거 같아</td>\n",
              "      <td>사랑은 쟁취하는 거예요.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Q                A\n",
              "8725         헤이진지 이제 딱 한달  후폭풍이 지나갔길 바랄게요.\n",
              "11697  친구들이 짝녀를 별로라고 하네요.   무슨 이유에서 그랬을까요.\n",
              "4928         피자 막막 먹고 싶어.         배달 시키세요.\n",
              "2172              불 날 뻔했어           조심하세요.\n",
              "11707  친구의 남친 좋아하게 된 거 같아    사랑은 쟁취하는 거예요."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"./dataset/ChatbotData.csv\")\n",
        "dataset.drop((\"label\"), axis=\"columns\", inplace=True)\n",
        "\n",
        "display(dataset.sample(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "growing-onion",
      "metadata": {
        "id": "growing-onion",
        "outputId": "bf9c1973-cdc2-427c-b62b-666f02fe2dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11823 entries, 0 to 11822\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       11823 non-null  object\n",
            " 1   A       11823 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 184.9+ KB\n",
            "Dupliacted Data Num: 73\n"
          ]
        }
      ],
      "source": [
        "dataset.info()\n",
        "dup = dataset.duplicated()\n",
        "dup = dup.value_counts()[True]\n",
        "\n",
        "print(f\"Dupliacted Data Num: {dup:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precious-services",
      "metadata": {
        "id": "precious-services"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ultimate-factory",
      "metadata": {
        "id": "ultimate-factory",
        "outputId": "10b9f661-e025-4465-b630-2a82ea561f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Num: 11,750\n"
          ]
        }
      ],
      "source": [
        "dataset.drop_duplicates(inplace=True)\n",
        "print(f\"Data Num: {len(dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "requested-spending",
      "metadata": {
        "id": "requested-spending"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([0-9?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎ가-힣0-9?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "\n",
        "dataset[\"Q\"] = dataset[\"Q\"].apply(preprocess_sentence)\n",
        "dataset[\"A\"] = dataset[\"A\"].apply(preprocess_sentence)\n",
        "\n",
        "display(dataset.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "returning-label",
      "metadata": {
        "id": "returning-label"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### Mecab을 이용한 형태소 기준 문장 분할\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "macro-shirt",
      "metadata": {
        "id": "macro-shirt",
        "outputId": "6cbe3b7a-5e70-4c94-a086-12fcedc6beca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1771</th>\n",
              "      <td>문 안 열림</td>\n",
              "      <td>힘껏 밀 어 보 세요 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6234</th>\n",
              "      <td>마지막 이 겠 죠</td>\n",
              "      <td>마지막 이 아닐 지도 몰라요 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4301</th>\n",
              "      <td>직구 로 사 야지</td>\n",
              "      <td>좋 은 거 사 나 봐요 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8779</th>\n",
              "      <td>회복 되 는 게 하루하루 가 다른 거 같 아 ! !</td>\n",
              "      <td>회복력 이 좋 으시 군요 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9210</th>\n",
              "      <td>나이 먹 고 짝사랑 하 는 내 가 한심 해 .</td>\n",
              "      <td>사랑 에 는 나이 가 상관 없 어요 .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Q                      A\n",
              "1771                        문 안 열림          힘껏 밀 어 보 세요 .\n",
              "6234                     마지막 이 겠 죠      마지막 이 아닐 지도 몰라요 .\n",
              "4301                     직구 로 사 야지         좋 은 거 사 나 봐요 .\n",
              "8779  회복 되 는 게 하루하루 가 다른 거 같 아 ! !        회복력 이 좋 으시 군요 .\n",
              "9210     나이 먹 고 짝사랑 하 는 내 가 한심 해 .  사랑 에 는 나이 가 상관 없 어요 ."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "m = Mecab()\n",
        "\n",
        "dataset[\"Q\"] = dataset[\"Q\"].apply(lambda x: \" \".join(m.morphs(x)))\n",
        "dataset[\"A\"] = dataset[\"A\"].apply(lambda x: \" \".join(m.morphs(x)))\n",
        "\n",
        "display(dataset.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intellectual-vampire",
      "metadata": {
        "id": "intellectual-vampire"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 디코더 문장 &lt;SOS>, &lt;EOS> 토큰 추가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ambient-steal",
      "metadata": {
        "id": "ambient-steal",
        "outputId": "8430ab7d-0699-47da-b137-c46bf407abcf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>남자 친구 는 어디 서 만나</td>\n",
              "      <td>&lt;sos&gt; 원 하 는 사람 이 있 는 장소 에 가 보 세요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8618</th>\n",
              "      <td>헤어진지 1 년 .</td>\n",
              "      <td>&lt;sos&gt; 아직 도 힘들 지 않 았 으면 좋 겠 어요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11663</th>\n",
              "      <td>첫 사랑 을 추억 해</td>\n",
              "      <td>&lt;sos&gt; 첫 사랑 은 항상 추억 의 대상 이 죠 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2632</th>\n",
              "      <td>술 좀 그만 마셔야 지</td>\n",
              "      <td>&lt;sos&gt; 술 은 적당히 즐기 세요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10489</th>\n",
              "      <td>어떻게 여러 명 을 좋아할 수 있 어 ?</td>\n",
              "      <td>&lt;sos&gt; 저 도 이해 는 안 갑니다 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Q                                         A\n",
              "749           남자 친구 는 어디 서 만나  <sos> 원 하 는 사람 이 있 는 장소 에 가 보 세요 . <eos>\n",
              "8618               헤어진지 1 년 .     <sos> 아직 도 힘들 지 않 았 으면 좋 겠 어요 . <eos>\n",
              "11663             첫 사랑 을 추억 해       <sos> 첫 사랑 은 항상 추억 의 대상 이 죠 . <eos>\n",
              "2632             술 좀 그만 마셔야 지               <sos> 술 은 적당히 즐기 세요 . <eos>\n",
              "10489  어떻게 여러 명 을 좋아할 수 있 어 ?              <sos> 저 도 이해 는 안 갑니다 . <eos>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset[\"A\"] = dataset[\"A\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")\n",
        "\n",
        "display(dataset.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "christian-sample",
      "metadata": {
        "id": "christian-sample"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 데이터 토큰화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "otherwise-ontario",
      "metadata": {
        "id": "otherwise-ontario"
      },
      "source": [
        "#### 토크나이저 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "offensive-tower",
      "metadata": {
        "id": "offensive-tower",
        "outputId": "4aafdcd4-41e2-4c01-bafa-c758f29fbf42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer Vocab Size: 6,810\n"
          ]
        }
      ],
      "source": [
        "def get_tokenizer(corpus, vocab_size):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='',\n",
        "        oov_token=\"<UNK>\",\n",
        "        num_words=vocab_size\n",
        "    )\n",
        "    corpus_input = [sentence.split() for sentence in corpus]\n",
        "    tokenizer.fit_on_texts(corpus_input)\n",
        "\n",
        "    if vocab_size is not None:\n",
        "        words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1]\n",
        "        for w in words_frequency:\n",
        "            del tokenizer.word_index[w]\n",
        "            del tokenizer.word_counts[w]\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "concat = pd.concat([dataset[\"Q\"], dataset[\"A\"]])\n",
        "tokenizer = get_tokenizer(concat, None)\n",
        "\n",
        "print(\"Tokenizer Vocab Size:\", f\"{len(tokenizer.word_index):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "floppy-siemens",
      "metadata": {
        "id": "floppy-siemens"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 문장 정수화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unavailable-balance",
      "metadata": {
        "id": "unavailable-balance"
      },
      "outputs": [],
      "source": [
        "def encoding_sentence(copus, tokenizer):\n",
        "    tensor = tokenizer.texts_to_sequences(copus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor, padding='post'\n",
        "    )\n",
        "    return tensor\n",
        "\n",
        "\n",
        "enc_tensor = encoding_sentence(dataset[\"Q\"], tokenizer)\n",
        "dec_tensor = encoding_sentence(dataset[\"A\"], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "successful-mainland",
      "metadata": {
        "id": "successful-mainland"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 토크나이저 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "architectural-removal",
      "metadata": {
        "id": "architectural-removal",
        "outputId": "c41edf2f-a169-4693-f66f-be1adf42dd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer Vocab Size: 5,872\n"
          ]
        }
      ],
      "source": [
        "\n",
        "concat = pd.concat([dataset[\"Q\"], dataset[\"A\"]])\n",
        "tokenizer = get_tokenizer(concat, 5872)\n",
        "\n",
        "\n",
        "\n",
        "#문장 길이\n",
        "q = dataset[\"Q\"].apply(lambda x: len(tokenizer.texts_to_sequences([x])[0]) <= 15)\n",
        "a = dataset[\"A\"].apply(lambda x: len(tokenizer.texts_to_sequences([x])[0]) <= 18)\n",
        "dataset = dataset[q & a]\n",
        "\n",
        "\n",
        "\n",
        "print(\"Tokenizer Vocab Size:\", f\"{len(tokenizer.word_index):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "respected-period",
      "metadata": {
        "id": "respected-period"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 테스트 데이터 분할\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impressed-officer",
      "metadata": {
        "id": "impressed-officer",
        "outputId": "1022767b-4f30-4469-bc5f-1143736a3c38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>같이 놀 러 갈 친구 가 없 어</td>\n",
              "      <td>&lt;sos&gt; 혼자 도 좋 아요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>개념 도 놓 고 옴</td>\n",
              "      <td>&lt;sos&gt; 그게 제일 중요 한 건데요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>강아지 키우 고 싶 어</td>\n",
              "      <td>&lt;sos&gt; 책임 질 수 있 을 때 키워 보 세요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>같이 할 수 있 는 취미 생활 뭐 있 을까</td>\n",
              "      <td>&lt;sos&gt; 함께 하 면 서로 를 더 많이 알 게 될 거 예요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>개념 이 없 어</td>\n",
              "      <td>&lt;sos&gt; 그게 제일 중요 한 건데요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Q                                          A\n",
              "74        같이 놀 러 갈 친구 가 없 어                    <sos> 혼자 도 좋 아요 . <eos>\n",
              "87               개념 도 놓 고 옴               <sos> 그게 제일 중요 한 건데요 . <eos>\n",
              "68             강아지 키우 고 싶 어         <sos> 책임 질 수 있 을 때 키워 보 세요 . <eos>\n",
              "79  같이 할 수 있 는 취미 생활 뭐 있 을까  <sos> 함께 하 면 서로 를 더 많이 알 게 될 거 예요 . <eos>\n",
              "88                 개념 이 없 어               <sos> 그게 제일 중요 한 건데요 . <eos>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_dataset = dataset[:100]\n",
        "dataset = dataset[100:]\n",
        "\n",
        "display(test_dataset.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vietnamese-diana",
      "metadata": {
        "id": "vietnamese-diana"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 데이터 증강\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rising-division",
      "metadata": {
        "id": "rising-division"
      },
      "source": [
        "#### 한국어 Word2Vec 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fabulous-manchester",
      "metadata": {
        "id": "fabulous-manchester"
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.Word2Vec.load('./dataset/ko.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chemical-permit",
      "metadata": {
        "id": "chemical-permit"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 데이터 증강 함수 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "separated-yorkshire",
      "metadata": {
        "id": "separated-yorkshire"
      },
      "outputs": [],
      "source": [
        "#Lexical Substitution\n",
        "def lexical_sub(sentence, word2vec, enc_arg=True):\n",
        "    toks = sentence.split()\n",
        "    if not enc_arg:   #<sos>, <eos> 토큰 제외\n",
        "        toks = toks[1:-1]\n",
        "\n",
        "    _from = random.choice(toks)\n",
        "\n",
        "    try:\n",
        "        _to = word2vec.most_similar(_from)[0][0]\n",
        "    except:\n",
        "        return \"_\"\n",
        "\n",
        "    res = \"\"\n",
        "    for tok in sentence.split():\n",
        "        if tok == _from:\n",
        "            res += _to + \" \"\n",
        "        else:\n",
        "            res += tok + \" \"\n",
        "    return res\n",
        "\n",
        "\n",
        "#Question, Answer에 따른 데이터 증강 함수\n",
        "def argument_data(dataset, word2vec, enc_arg=True):\n",
        "    qna = \"Q\" if enc_arg else \"A\"\n",
        "    arg = dataset[qna].apply(lambda x: lexical_sub(x, word2vec, enc_arg))\n",
        "\n",
        "    arg_data = dataset.copy()\n",
        "    arg_data[qna] = arg\n",
        "\n",
        "    arg_data = arg_data[arg_data[qna] != \"_\"]\n",
        "    return arg_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "motivated-physics",
      "metadata": {
        "id": "motivated-physics"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 데이터 증강 수행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "executed-czech",
      "metadata": {
        "id": "executed-czech",
        "outputId": "06799720-da88-4781-dd21-a4be4d5114c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question Sentence: 걱정 좀 없이 살 고 싶 다 . ======> 걱정 조금 없이 살 고 싶 다 . \n",
            "Answer Sentence: <sos> 누구 나 걱정 은 있 어요 . <eos> ======> <sos> 누구 나 걱정 은 있 는데요 . <eos> \n"
          ]
        }
      ],
      "source": [
        "enc_alpha = argument_data(dataset, w2v, True)\n",
        "dec_alpha = argument_data(dataset, w2v, False)\n",
        "\n",
        "\n",
        "enc_idx = set(dataset.index)\n",
        "enc_alpha_idx = set(enc_alpha.index)\n",
        "dec_alpha_idx = set(dec_alpha.index)\n",
        "\n",
        "vet = enc_idx & enc_alpha_idx & dec_alpha_idx\n",
        "vet = list(vet)[0]\n",
        "\n",
        "print(f\"Question Sentence: {dataset['Q'][vet]} ======> {enc_alpha['Q'][vet]}\")\n",
        "print(f\"Answer Sentence: {dataset['A'][vet]} ======> {dec_alpha['A'][vet]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amber-raising",
      "metadata": {
        "id": "amber-raising"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 기존 데이터와 증강 데이터 합치기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "infrared-hometown",
      "metadata": {
        "id": "infrared-hometown",
        "outputId": "b4d227a9-3e7e-4eb7-b9d8-804dd6a8fe27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Num: 29,951\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9539</th>\n",
              "      <td>내일 대관식 이 야</td>\n",
              "      <td>&lt;sos&gt; 떨리 겠 어요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>내 사랑 은 어디 있 나</td>\n",
              "      <td>&lt;sos&gt; 같 은 하늘 아래 묻히 에 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2311</th>\n",
              "      <td>산 뛰어넘 어 산 이 네</td>\n",
              "      <td>&lt;sos&gt; 그래도 넘 을 수 있 을 거 예요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7158</th>\n",
              "      <td>오늘 도 보 고 왔 어서</td>\n",
              "      <td>&lt;sos&gt; 그것 이 최선 의 선택 일거 라 생각 해요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>건강 관리</td>\n",
              "      <td>&lt;sos&gt; 운동 을 해의 보 세요 . &lt;eos&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Q                                      A\n",
              "9539     내일 대관식 이 야                   <sos> 떨리 겠 어요 . <eos>\n",
              "882    내 사랑 은 어디 있 나          <sos> 같 은 하늘 아래 묻히 에 . <eos> \n",
              "2311  산 뛰어넘 어 산 이 네        <sos> 그래도 넘 을 수 있 을 거 예요 . <eos>\n",
              "7158  오늘 도 보 고 왔 어서   <sos> 그것 이 최선 의 선택 일거 라 생각 해요 . <eos>\n",
              "107            건강 관리            <sos> 운동 을 해의 보 세요 . <eos> "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = pd.concat([dataset, enc_alpha, dec_alpha])\n",
        "dataset = dataset.sample(frac=1)\n",
        "\n",
        "print(f\"Dataset Num: {len(dataset):,}\")\n",
        "display(dataset[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stylish-miller",
      "metadata": {
        "id": "stylish-miller"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 문장 정수 인코딩 수행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "renewable-embassy",
      "metadata": {
        "id": "renewable-embassy",
        "outputId": "6ca0f097-3886-47ac-90e5-37254d3b815e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data num: 29,951\n"
          ]
        }
      ],
      "source": [
        "enc_tensor = encoding_sentence(dataset[\"Q\"], tokenizer)\n",
        "dec_tensor = encoding_sentence(dataset[\"A\"], tokenizer)\n",
        "\n",
        "print(\"Data num:\", f\"{len(enc_tensor):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "respiratory-fashion",
      "metadata": {
        "id": "respiratory-fashion"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Transformer 모델 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "offshore-williams",
      "metadata": {
        "id": "offshore-williams"
      },
      "outputs": [],
      "source": [
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i)/d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)\n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask\n",
        "        )\n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-farmer",
      "metadata": {
        "id": "descending-farmer"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, enc_attn\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "organic-characterization",
      "metadata": {
        "id": "organic-characterization"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "                        for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "\n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "\n",
        "        return out, enc_attns\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "                            for _ in range(n_layers)]\n",
        "\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "\n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fifty-edition",
      "metadata": {
        "id": "fifty-edition"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers, d_model, n_heads, d_ff,\n",
        "        src_vocab_size, tgt_vocab_size,\n",
        "        pos_len,\n",
        "        dropout=0.2,\n",
        "        shared=True\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared = shared\n",
        "\n",
        "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "\n",
        "        logits = self.fc(dec_out)\n",
        "\n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocational-sally",
      "metadata": {
        "id": "vocational-sally"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 모델 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-living",
      "metadata": {
        "id": "dangerous-living"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    n_layers=2,\n",
        "    d_model=128,\n",
        "    n_heads=8,\n",
        "    d_ff=128,\n",
        "    dropout=0.5,\n",
        "    pos_len=200,\n",
        "    shared=True,\n",
        "    src_vocab_size=5872, tgt_vocab_size=5872\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "popular-elements",
      "metadata": {
        "id": "popular-elements"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mechanical-experiment",
      "metadata": {
        "id": "mechanical-experiment"
      },
      "outputs": [],
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
        ")\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "urban-collection",
      "metadata": {
        "id": "urban-collection"
      },
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
        "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "northern-infrared",
      "metadata": {
        "id": "northern-infrared"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 모델 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rural-clinton",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bdc792caaf584f638ea2286a4c7d3d08",
            "df09fa10320b4c4ba904ca4c8e8359ae",
            "3dcfd946346142f6ab0c918adaf8a88f",
            "22399e7264e743a8b9459f0784461b1e",
            "333f3dbd0eb64f3e947a6512a063e74f",
            "3320813624874f4facc24ef0a9f4ed25",
            "35c34a3d606346cea1fab0731098d434",
            "c396961951544bf4b93f2d80d24286ce",
            "a8016e57776845afb96d2340a973153f",
            "d24ee1892c6844399a4d001bc9a58fbf"
          ]
        },
        "id": "rural-clinton",
        "outputId": "4c09deba-b795-4b0b-f392-8ee6c6fb6f94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdc792caaf584f638ea2286a4c7d3d08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df09fa10320b4c4ba904ca4c8e8359ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dcfd946346142f6ab0c918adaf8a88f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22399e7264e743a8b9459f0784461b1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "333f3dbd0eb64f3e947a6512a063e74f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3320813624874f4facc24ef0a9f4ed25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35c34a3d606346cea1fab0731098d434",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c396961951544bf4b93f2d80d24286ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8016e57776845afb96d2340a973153f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d24ee1892c6844399a4d001bc9a58fbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/468 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def model_fit(enc_train, dec_train, model, epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        idx_list = list(range(0, enc_train.shape[0], batch_size))\n",
        "        random.shuffle(idx_list)\n",
        "        t = tqdm(idx_list)\n",
        "\n",
        "        for (batch, idx) in enumerate(t):\n",
        "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "            train_step(\n",
        "                enc_train[idx:idx+batch_size],\n",
        "                dec_train[idx:idx+batch_size],\n",
        "                model,\n",
        "                optimizer\n",
        "            )\n",
        "\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
        "\n",
        "\n",
        "model_fit(enc_tensor, dec_tensor, transformer, epochs=10, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과 확인"
      ],
      "metadata": {
        "id": "xf7ecAsJe5jW"
      },
      "id": "xf7ecAsJe5jW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incident-perth",
      "metadata": {
        "id": "incident-perth",
        "outputId": "d12a7160-1f44-4bfe-b48b-27a8433683e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Quenstion: 지루하다, 놀러가고 싶어.                \tAnswer: 시간 이 필요 한가 봐요 .               \n",
            "Quenstion: 오늘 일찍 일어났더니 피곤하다.             \tAnswer: 아침 일 이 많 았 나 봅니다 .            \n",
            "Quenstion: 간만에 여자친구랑 데이트 하기로 했어.         \tAnswer: 좋 은 친구 가 여기 가 여기 가 길 바랄게요 .   \n",
            "Quenstion: 집에 있는다는 소리야.                  \tAnswer: 익숙 <UNK> 을 <UNK> 군요 .         \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "def translate(sentence, model, tokenizer, enc_tensor, dec_tensor):\n",
        "    enc_maxlen = enc_tensor.shape[-1]\n",
        "    dec_maxlen = dec_tensor.shape[-1]\n",
        "\n",
        "    sos_idx = tokenizer.word_index['<sos>']\n",
        "    eos_idx = tokenizer.word_index['<eos>']\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    m = Mecab()\n",
        "    sentence = m.morphs(sentence)\n",
        "\n",
        "    _input = tokenizer.texts_to_sequences([sentence])\n",
        "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        _input,\n",
        "        maxlen=enc_maxlen,\n",
        "        padding='post'\n",
        "    )\n",
        "\n",
        "    ids = []\n",
        "    output = tf.expand_dims([sos_idx], 0)\n",
        "\n",
        "    for i in range(dec_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
        "            _input, output\n",
        "        )\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "            _input, output, enc_padding_mask, combined_mask, dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predicted_id = tf.argmax(\n",
        "            tf.math.softmax(predictions, axis=-1)[0, -1]\n",
        "        ).numpy().item()\n",
        "\n",
        "        if predicted_id == eos_idx:\n",
        "            result = tokenizer.sequences_to_texts([ids])\n",
        "            return result\n",
        "\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "    result = tokenizer.sequences_to_texts([ids])\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"=\" * 100)\n",
        "test_sentences = [\n",
        "    \"지루하다, 놀러가고 싶어.\",\n",
        "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
        "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
        "    \"집에 있는다는 소리야.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    ans = translate(sentence, transformer, tokenizer, enc_tensor, dec_tensor)[0]\n",
        "    print(f\"Quenstion: {sentence:<30}\\tAnswer: {ans:<30}\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "animated-webcam",
      "metadata": {
        "id": "animated-webcam"
      },
      "source": [
        "<br>\n",
        "\n",
        "## 모델 평가\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wound-wagner",
      "metadata": {
        "id": "wound-wagner"
      },
      "source": [
        "#### Beam Search 및 BLEU 계산 함수 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "severe-moisture",
      "metadata": {
        "id": "severe-moisture"
      },
      "outputs": [],
      "source": [
        "#모델 입력 및 출력 함수\n",
        "def calc_prob(src_ids, tgt_ids, model):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
        "        src_ids, tgt_ids\n",
        "    )\n",
        "\n",
        "    predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
        "        src_ids, tgt_ids, enc_padding_mask, combined_mask, dec_padding_mask\n",
        "    )\n",
        "    return tf.math.softmax(predictions, axis=-1)\n",
        "=\n",
        "def beam_search_decoder(\n",
        "    sentence, model, tokenizer,\n",
        "    enc_maxlen, dec_maxlen,\n",
        "    beam_size\n",
        "):\n",
        "    sos_idx = tokenizer.word_index['<sos>']\n",
        "    eos_idx = tokenizer.word_index['<eos>']\n",
        "\n",
        "    tokens = tokenizer.texts_to_sequences([sentence])\n",
        "    src_in = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokens,\n",
        "        maxlen=enc_maxlen,\n",
        "        padding='post'\n",
        "    )\n",
        "\n",
        "    pred_cache = np.zeros((beam_size * beam_size, dec_maxlen), dtype=np.long)\n",
        "    pred = np.zeros((beam_size, dec_maxlen), dtype=np.long)\n",
        "\n",
        "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
        "    scores = np.ones((beam_size, ))\n",
        "\n",
        "    pred[:, 0] = sos_idx\n",
        "\n",
        "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
        "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
        "\n",
        "\n",
        "    for seq_pos in range(1, dec_maxlen):\n",
        "        score_cache = np.ones((beam_size * beam_size, ))\n",
        "\n",
        "        # init\n",
        "        for branch_idx in range(beam_size):\n",
        "            cache_pos = branch_idx*beam_size\n",
        "\n",
        "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
        "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
        "            pred[branch_idx, :seq_pos]\n",
        "\n",
        "        for branch_idx in range(beam_size):\n",
        "            cache_pos = branch_idx*beam_size\n",
        "\n",
        "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
        "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
        "                dec_in = tf.expand_dims(dec_in, 0)\n",
        "\n",
        "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
        "\n",
        "            for beam_idx in range(beam_size):\n",
        "                max_idx = np.argmax(prob)\n",
        "\n",
        "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
        "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
        "\n",
        "                prob[max_idx] = -1\n",
        "\n",
        "        for beam_idx in range(beam_size):\n",
        "            if eos_flag[beam_idx] == -1: continue\n",
        "\n",
        "            max_idx = np.argmax(score_cache)\n",
        "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
        "\n",
        "            pred[beam_idx, :seq_pos+1] = prediction\n",
        "            scores[beam_idx] = score_cache[max_idx]\n",
        "            score_cache[max_idx] = -1\n",
        "\n",
        "            if prediction[-1] == eos_idx:\n",
        "                eos_flag[beam_idx] = -1\n",
        "    return pred\n",
        "\n",
        "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
        "    return sentence_bleu(\n",
        "        [reference],\n",
        "        candidate,\n",
        "        weights=weights,\n",
        "        smoothing_function=SmoothingFunction().method1\n",
        "    )\n",
        "\n",
        "def beam_bleu(reference, ids, tokenizer, verbose=False):\n",
        "    reference = reference.split()\n",
        "\n",
        "    total_score = 0.0\n",
        "    for _id in ids:\n",
        "        seq2text = tokenizer.sequences_to_texts([_id])[0]\n",
        "        _idx =  seq2text.find(\"<eos>\")\n",
        "        seq2text = seq2text[6:_idx]\n",
        "        candidate = seq2text.split()\n",
        "        score = calculate_bleu(reference, candidate)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"=\" * 100)\n",
        "            print(\"Reference:\".ljust(10), \" \".join(reference))\n",
        "            print(\"Candidate:\".ljust(10), \" \".join(candidate), end=\"\\n\\n\")\n",
        "            print(\"BLEU:\".ljust(10), f\"{calculate_bleu(reference, candidate):.3f}\")\n",
        "            print(\"=\" * 100, end=\"\\n\\n\")\n",
        "\n",
        "        total_score += score\n",
        "\n",
        "    return total_score / len(ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ready-carroll",
      "metadata": {
        "id": "ready-carroll"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 예문의 Beam search 문장과 BLEU 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usual-hebrew",
      "metadata": {
        "id": "usual-hebrew",
        "outputId": "e410acda-15f2-46b7-9bb7-fb2f7d5e9442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Reference: 땀 을 식혀 주 세요 .\n",
            "Candidate: 잘 하 셨 어요 .\n",
            "\n",
            "BLEU:      0.044\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Reference: 땀 을 식혀 주 세요 .\n",
            "Candidate: 잘 하 셨 어요 는데\n",
            "\n",
            "BLEU:      0.000\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Reference: 땀 을 식혀 주 세요 .\n",
            "Candidate: 잘 하 셨 나 .\n",
            "\n",
            "BLEU:      0.044\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Reference: 땀 을 식혀 주 세요 .\n",
            "Candidate: 잘 <UNK> 셨 어요 .\n",
            "\n",
            "BLEU:      0.044\n",
            "====================================================================================================\n",
            "\n",
            "====================================================================================================\n",
            "Reference: 땀 을 식혀 주 세요 .\n",
            "Candidate: 잘 하 셨 나 는데\n",
            "\n",
            "BLEU:      0.000\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "idx = 15\n",
        "test_enc_sentence = test_dataset[\"Q\"][idx]\n",
        "\n",
        "test_dec_tensor = encoding_sentence(test_dataset[\"A\"], tokenizer)\n",
        "test_dec_sentence = tokenizer.sequences_to_texts([test_dec_tensor[idx]])[0]\n",
        "_idx = test_dec_sentence.find(\"<eos>\")\n",
        "test_dec_sentence = test_dec_sentence[6:_idx]\n",
        "\n",
        "\n",
        "ids = beam_search_decoder(\n",
        "    test_enc_sentence,\n",
        "    transformer, tokenizer,\n",
        "    enc_tensor.shape[-1], dec_tensor.shape[-1],\n",
        "    beam_size=5\n",
        ")\n",
        "\n",
        "bleu = beam_bleu(test_dec_sentence, ids, tokenizer, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "infinite-notebook",
      "metadata": {
        "id": "infinite-notebook"
      },
      "source": [
        "<br>\n",
        "\n",
        "#### 테스트 데이터 BLEU 구하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proprietary-destruction",
      "metadata": {
        "id": "proprietary-destruction",
        "outputId": "8dae7e23-c6d4-4b48-acc5-6a684d9c54d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "Test Data BLEU: 0.419\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "enc_maxlen = enc_tensor.shape[-1]\n",
        "dec_maxlen = dec_tensor.shape[-1]\n",
        "\n",
        "aver_bleu = 0\n",
        "for _, que, ans in test_dataset.itertuples():\n",
        "    ids = beam_search_decoder(\n",
        "        que,\n",
        "        transformer, tokenizer,\n",
        "        enc_maxlen, dec_maxlen,\n",
        "        beam_size=5\n",
        "    )\n",
        "\n",
        "    test_dec_sentence = tokenizer.sequences_to_texts([que])[0]\n",
        "    _idx = test_dec_sentence.find(\"<eos>\")\n",
        "    test_dec_sentence = test_dec_sentence[6:_idx]\n",
        "\n",
        "    aver_bleu += beam_bleu(test_dec_sentence, ids, tokenizer, verbose=False)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(f\"Test Data BLEU: {aver_bleu:.3f}\")\n",
        "print(\"=\" * 100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}