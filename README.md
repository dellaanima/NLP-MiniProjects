# NLP GoingDeeper
## CONTENTS 

|N|Title|Contents|
|:---:|:---:|---|
|1|<b>텍스트 데이터 다루기</b>|다양한 텍스트 데이터 전처리 기법을 소개. Word나 형태소 레벨의 tokenizer 및 subword 레벨 tokenizing 기법(BPE, sentencepiece) 학습|
|2|<b>멋진 단어사전 만들기</b>|<b>[PROJECT]</b> 단어사전을 만들어보고 이를 토대로 perplexity를 측정해보는 프로젝트|
|3|<b>텍스트의 분포로 벡터화 하기</b>|텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA)|
|4|<b>뉴스 카테고리 다중분류</b>|<b>[PROJECT]</b> 뉴스 텍스트의 주제를 분류하는 task를 다양한 기법으로 시도해보고 비교, 분석 하는 프로젝트|
|5|<b>워드 임베딩</b>|워드 임베딩 벡터(Word2Vec, FastText, Glove)의 원리와 사용법을 학습|
|6|<b>WEAT</b>|<b>[PROJECT]</b> WEAT(Word Embedding Association Test) 기법으로, Word Embedding Model 의 편향성 측정|
|7|<b>Seq2seq와 Attention</b>|언어 모델이 발전해 온 과정에 대해 배우고, Seq2seq에 대해 학습|
|8|<b>Seq2seq으로 번역기 만들기</b>|Attention 기법을 추가하여 Seq2seq 기반의 번역기 성능을 높여보기|
|9|<b>Transformer가 나오기까지</b>|Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계|
|10|<b>Transformer로 번역기 만들기</b>|트랜스포머를 이용해 번역기를 만드는 프로젝트|
|11|<b>기계 번역이 걸어온 길</b>|자연어 처리에서 Data Augmentation은 어떻게 하는지, 자연어 처리 성능은 어떻게 측정할 수 있는지 학습|
|12|<b>번역가는 대화에도 능하다</b>|다양한 디코딩 방식을 활용해 모델 구현 후 BLEU Score를 이용하여 성능 평가, 한국어 챗봇 구현 프로젝트 수행|
|13|<b>modern NLP의 흐름에 올라타보자</b>|트랜스포머를 바탕으로 한 최근 NLP 모델에 대해 학습|
|14|<b>BERT pretrained model 제작</b>|대표적인 pretrained language model인 BERT 원리에 대해 학습|
|15|<b>NLP Framework의 활용</b>|최다양한 NLP Framework에 대해 학습하고, Huggingface transformer를 중심으로 설계구조와 활용법 학습|
|16|<b>HuggingFace 커스텀 프로젝트 만들기</b>|Huggingface transformer를 활용한 커스텀 프로젝트 수행|
|16|<b>HuggingFace 커스텀 프로젝트 만들기</b>|Huggingface transformer를 활용한 커스텀 프로젝트 수행|
|17|<b>#NLP Trend Note 1</b>|최신 LLM 소개, InstructGPT의 SFT, RM, PPO 학습 메커니즘 소개|
|18|<b>#NLP Trend Note 2</b>|KochatGPT 구현|


